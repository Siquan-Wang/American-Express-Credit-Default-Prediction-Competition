{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b757587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fe [189]\n",
      "Index(['S_3_sum', 'S_5_sum', 'S_6_sum', 'S_7_sum', 'S_8_sum', 'S_9_sum', 'S_11_sum', 'S_12_sum', 'S_13_sum', 'S_15_sum', 'S_16_sum', 'S_17_sum', 'S_18_sum', 'S_19_sum', 'S_20_sum', 'S_22_sum', 'S_23_sum', 'S_24_sum', 'S_25_sum', 'S_26_sum', 'S_27_sum', 'P_2_sum', 'P_3_sum', 'P_4_sum', 'D_39_sum', 'D_41_sum', 'D_42_sum', 'D_45_sum', 'D_46_sum', 'D_48_sum', 'D_50_sum', 'D_51_sum', 'D_53_sum', 'D_55_sum', 'D_56_sum', 'D_58_sum', 'D_59_sum', 'D_60_sum', 'D_62_sum', 'D_70_sum', 'D_71_sum', 'D_74_sum', 'D_75_sum', 'D_78_sum', 'D_83_sum', 'D_102_sum', 'D_112_sum', 'D_113_sum', 'D_115_sum', 'D_118_sum', 'D_119_sum', 'D_121_sum', 'D_122_sum', 'D_128_sum', 'D_132_sum', 'D_140_sum', 'D_141_sum', 'D_144_sum', 'D_145_sum', 'B_1_sum', 'B_2_sum', 'B_3_sum', 'B_5_sum', 'B_6_sum', 'B_7_sum', 'B_8_sum', 'B_9_sum', 'B_10_sum', 'B_11_sum', 'B_12_sum', 'B_13_sum', 'B_14_sum', 'B_15_sum', 'B_17_sum', 'B_18_sum', 'B_21_sum', 'B_23_sum', 'B_24_sum', 'B_25_sum', 'B_26_sum', 'B_27_sum', 'B_28_sum', 'B_36_sum',\n",
      "       'B_37_sum', 'B_40_sum'],\n",
      "      dtype='object')\n",
      "Stats Sum calc [189]\n",
      "P-S feature added\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addding col-mean 2375 cols ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42', 'D_43', 'D_44', 'B_4', 'D_45', 'B_5', 'R_2', 'D_46', 'D_47', 'D_48', 'D_49', 'B_6', 'B_7', 'B_8', 'D_50', 'D_51', 'B_9', 'R_3', 'D_52', 'P_3', 'B_10', 'D_53', 'S_5', 'B_11', 'S_6', 'D_54', 'R_4', 'S_7', 'B_12', 'S_8', 'D_55', 'D_56', 'B_13', 'R_5', 'D_58', 'S_9', 'B_14', 'D_59', 'D_60', 'D_61', 'B_15', 'S_11', 'D_62', 'D_65', 'B_16', 'B_17', 'B_18', 'B_19', 'B_20', 'S_12', 'R_6', 'S_13', 'B_21', 'D_69', 'B_22', 'D_70', 'D_71', 'D_72', 'S_15', 'B_23', 'D_73', 'P_4', 'D_74', 'D_75', 'D_76', 'B_24', 'R_7', 'D_77', 'B_25', 'B_26', 'D_78', 'D_79', 'R_8', 'R_9', 'S_16', 'D_80', 'R_10', 'R_11', 'B_27', 'D_81', 'D_82', 'S_17', 'R_12', 'B_28', 'R_13', 'D_83', 'R_14', 'R_15', 'D_84', 'R_16', 'B_29', 'S_18', 'D_86', 'D_87', 'R_17', 'R_18', 'D_88', 'B_31', 'S_19', 'R_19', 'B_32', 'S_20', 'R_20', 'R_21', 'B_33', 'D_89', 'R_22', 'R_23', 'D_91', 'D_92', 'D_93', 'D_94', 'R_24', 'R_25', 'D_96', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'D_102', 'D_104', 'D_105', 'D_106', 'D_107', 'B_36', 'B_37', 'R_26', 'R_27', 'D_108', 'D_109', 'D_110', 'D_111', 'B_39', 'D_112', 'B_40', 'S_27', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127', 'D_128', 'D_129', 'B_41', 'B_42', 'D_130', 'D_131', 'D_132', 'D_133', 'R_28', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145']\n",
      "Dropping ['S_3_sum', 'S_5_sum', 'S_6_sum', 'S_7_sum', 'S_8_sum', 'S_9_sum', 'S_11_sum', 'S_12_sum', 'S_13_sum', 'S_15_sum', 'S_16_sum', 'S_17_sum', 'S_18_sum', 'S_19_sum', 'S_20_sum', 'S_22_sum', 'S_23_sum', 'S_24_sum', 'S_25_sum', 'S_26_sum', 'S_27_sum', 'P_2_sum', 'P_3_sum', 'P_4_sum', 'D_39_sum', 'D_41_sum', 'D_42_sum', 'D_45_sum', 'D_46_sum', 'D_48_sum', 'D_50_sum', 'D_51_sum', 'D_53_sum', 'D_55_sum', 'D_56_sum', 'D_58_sum', 'D_59_sum', 'D_60_sum', 'D_62_sum', 'D_70_sum', 'D_71_sum', 'D_74_sum', 'D_75_sum', 'D_78_sum', 'D_83_sum', 'D_102_sum', 'D_112_sum', 'D_113_sum', 'D_115_sum', 'D_118_sum', 'D_119_sum', 'D_121_sum', 'D_122_sum', 'D_128_sum', 'D_132_sum', 'D_140_sum', 'D_141_sum', 'D_144_sum', 'D_145_sum', 'B_1_sum', 'B_2_sum', 'B_3_sum', 'B_5_sum', 'B_6_sum', 'B_7_sum', 'B_8_sum', 'B_9_sum', 'B_10_sum', 'B_11_sum', 'B_12_sum', 'B_13_sum', 'B_14_sum', 'B_15_sum', 'B_17_sum', 'B_18_sum', 'B_21_sum', 'B_23_sum', 'B_24_sum', 'B_25_sum', 'B_26_sum', 'B_27_sum', 'B_28_sum', 'B_36_sum', 'B_37_sum', 'B_40_sum']\n",
      "Addding col-mean + custom features 141 cols ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42', 'D_43', 'D_44', 'B_4', 'D_45', 'B_5', 'R_2', 'D_46', 'D_47', 'D_48', 'D_49', 'B_6', 'B_7', 'B_8', 'D_50', 'D_51', 'B_9', 'R_3', 'D_52', 'P_3', 'B_10', 'D_53', 'S_5', 'B_11', 'S_6', 'D_54', 'R_4', 'S_7', 'B_12', 'S_8', 'D_55', 'D_56', 'B_13', 'R_5', 'D_58', 'S_9', 'B_14', 'D_59', 'D_60', 'D_61', 'B_15', 'S_11', 'D_62', 'D_65', 'B_16', 'B_17', 'B_18', 'B_19', 'B_20', 'S_12', 'R_6', 'S_13', 'B_21', 'D_69', 'B_22', 'D_70', 'D_71', 'D_72', 'S_15', 'B_23', 'D_73', 'P_4', 'D_74', 'D_75', 'D_76', 'B_24', 'R_7', 'D_77', 'B_25', 'B_26', 'D_78', 'D_79', 'R_8', 'R_9', 'S_16', 'D_80', 'R_10', 'R_11', 'B_27', 'D_81', 'D_82', 'S_17', 'R_12', 'B_28', 'R_13', 'D_83', 'R_14', 'R_15', 'D_84', 'R_16', 'B_29', 'S_18', 'D_86', 'D_87', 'R_17', 'R_18', 'D_88', 'B_31', 'S_19', 'R_19', 'B_32', 'S_20', 'R_20', 'R_21', 'B_33', 'D_89', 'R_22', 'R_23', 'D_91', 'D_92', 'D_93', 'D_94', 'R_24', 'R_25', 'D_96', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'D_102', 'D_104', 'D_105', 'D_106', 'D_107', 'B_36', 'B_37', 'R_26', 'R_27', 'D_108', 'D_109', 'D_110', 'D_111', 'B_39', 'D_112', 'B_40', 'S_27', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127', 'D_128', 'D_129', 'B_41', 'B_42', 'D_130', 'D_131', 'D_132', 'D_133', 'R_28', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145']\n",
      "Saving test FE to file\n",
      "FE finished\n"
     ]
    }
   ],
   "source": [
    "# Imports and Constants\n",
    "import os,random \n",
    "import tqdm \n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pathlib\n",
    "import tqdm\n",
    "import time \n",
    "import time \n",
    "import torch, gc \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()    \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "class CFG:\n",
    "  seed = 42\n",
    "  INPUT = \"../input/amex-data-integer-dtypes-parquet-format\"\n",
    "  TRAIN = True \n",
    "  INFER = True\n",
    "  n_folds = 5\n",
    "  target ='target'\n",
    "  DEBUG= True \n",
    "  ADD_CAT = True\n",
    "  ADD_LAG = True \n",
    "  ADD_DIFF =  [1, 2]\n",
    "  ADD_MIDDLE = True\n",
    "  output_dir = \"./\"\n",
    "\n",
    "path = f'{CFG.INPUT}'  \n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(CFG.seed)    \n",
    "\n",
    "\n",
    "\n",
    "features_avg = ['S_2_wk','B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18',\n",
    "                'B_19', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_28', 'B_29', 'B_30', 'B_32', 'B_33', 'B_37', 'B_38', 'B_39', 'B_40', 'B_41', 'B_42',\n",
    "                'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_50', 'D_51', 'D_53', 'D_54', 'D_55', 'D_58', 'D_59', 'D_60', 'D_61', \n",
    "                'D_62', 'D_65', 'D_66', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_80', 'D_82', 'D_84', 'D_86', 'D_91', \n",
    "                'D_92', 'D_94', 'D_96', 'D_103', 'D_104', 'D_108', 'D_112', 'D_113', 'D_114', 'D_115', 'D_117', 'D_118', 'D_119', 'D_120', 'D_121', 'D_122', 'D_123',\n",
    "                'D_124', 'D_125', 'D_126', 'D_128', 'D_129', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_140', 'D_141', 'D_142', 'D_144', 'D_145',\n",
    "                'P_2', 'P_3', 'P_4', 'R_1', 'R_2', 'R_3', 'R_7', 'R_8', 'R_9', 'R_10', 'R_11', 'R_14', 'R_15', 'R_16', 'R_17', 'R_20', 'R_21', 'R_22', 'R_24', \n",
    "                'R_26', 'R_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_18', 'S_22', 'S_23', 'S_25', 'S_26']\n",
    "\n",
    "# Feature Engineering on credit risk\n",
    "spend_p=[ 'S_3',  'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27']\n",
    "balance_p = ['B_1', 'B_2', 'B_3',  'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15',  'B_17', 'B_18',  'B_21',   'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28',  'B_36', 'B_37',  'B_40',    ]\n",
    "payment_p = ['P_2', 'P_3', 'P_4']\n",
    "delq = ['D_39',\n",
    "                'D_41', 'D_42', 'D_45', 'D_46', 'D_48', 'D_50', 'D_51', 'D_53', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', 'D_71', 'D_74', \n",
    "                'D_75', 'D_78', 'D_83', 'D_102', 'D_112', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_128', 'D_132', 'D_140', 'D_141', 'D_144',\n",
    "                'D_145']  \n",
    "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
    "            'D_126', 'D_63', 'D_64', 'D_66', 'D_68']     \n",
    "\n",
    "cat_cols_avg = [col for col in cat_cols if col in features_avg]\n",
    "g_num_cols = []\n",
    "\n",
    "# ====================================================\n",
    "#              Feature Engineering\n",
    "# ====================================================\n",
    "def process_data(df):\n",
    "    df,dgs = preprocess(df) \n",
    "    df = df.drop_duplicates('customer_ID',keep='last')\n",
    "    for dg in dgs:\n",
    "        df = df.merge(dg, on='customer_ID', how='left')\n",
    "        # drop specific non impactful cols \n",
    "    del dgs; gc.collect()    \n",
    "             \n",
    "    diff_cols = [col for col in df.columns if col.endswith('_diff')]\n",
    "    df = df.drop(diff_cols,axis=1)\n",
    "    print(f\"All stats merged {len(df.columns)}\")   \n",
    "  \n",
    "    math_col = globals()['g_num_cols']\n",
    "    # More Lag Features\n",
    "    for col in spend_p+payment_p+balance_p:\n",
    "        for col_2 in ['min','max']: \n",
    "          if f\"{col}_{col_2}\" in df.columns:\n",
    "              df[f'{col}_{col_2}_lag_sub'] = df[f\"{col}_{col_2}\"] - df[col]\n",
    "              df[f'{col}_{col_2}_lag_div'] = df[f\"{col}_{col_2}\"] / df[col] \n",
    "    print(\"Added more lags\")\n",
    "\n",
    "    # add More custom features\n",
    "    df[\"P2B9\"] = df[\"P_2\"] / df[\"B_9\"] \n",
    "    math_col = globals()['g_num_cols']\n",
    "    for pcol in math_col:\n",
    "      if pcol+\"_mean\" in df.columns:  \n",
    "        df[f'{pcol}-mean'] = df[pcol] - df[pcol+\"_mean\"]  \n",
    "        df[f'{pcol}-div-mean'] = df[pcol] /df[pcol+\"_mean\"]\n",
    "      if (pcol+\"_min\" in df.columns) and (pcol+\"_max\" in df.columns):  \n",
    "        df[f'{pcol}_min_div_max'] = df[pcol+\"_min\"] / df[pcol+\"_max\"]  \n",
    "        df[f'{pcol}_min-max'] = df[pcol+\"_min\"] - df[pcol+\"_max\"]\n",
    "    print(f\"Addding col-mean {len(df.columns)} cols {math_col}\")     \n",
    "\n",
    "\n",
    "    # Dropping Sum\n",
    "    drop_col = [col for  col in df.columns if  ((\"sum\" in col))]\n",
    "    print(f\"Dropping {drop_col}\")\n",
    "    df=df.drop(drop_col,axis=1)   \n",
    "\n",
    "    print(f\"Addding col-mean + custom features {len(features_avg)} cols {globals()['g_num_cols']}\")    \n",
    "    return df\n",
    "\n",
    "   \n",
    "def preprocess(df):\n",
    "    df['row_id'] = np.arange(df.shape[0])\n",
    "    not_used = get_not_used()\n",
    "    # Drop cols https://www.kaggle.com/code/raddar/redundant-features-amex/notebook\n",
    "    df=df.drop([\"D_103\",\"D_139\"],axis=1)\n",
    "    num_cols = [col for col in df.columns if col not in cat_cols+not_used]   \n",
    "\n",
    "    globals()['g_num_cols'] = num_cols\n",
    "    for col in df.columns:\n",
    "        if col not in not_used+cat_cols:\n",
    "           df[col] = df[col].astype('float32').round(decimals=2).astype('float16') \n",
    "    print(f\"Starting fe [{len(df.columns)}]\") \n",
    "    dgs=add_stats_step(df, num_cols) # add summary statistics to features\n",
    "\n",
    "    train_stat = df.groupby(\"customer_ID\")[spend_p+payment_p+delq+balance_p].agg('sum')\n",
    "    train_stat.columns = [x+'_sum' for x in train_stat.columns]\n",
    "    print(train_stat.columns)\n",
    "    train_stat.reset_index(inplace = True)    \n",
    "    dgs.append(train_stat)\n",
    "    del train_stat; gc.collect() \n",
    "    print(f\"Stats Sum calc [{len(df.columns)}]\")       \n",
    " \n",
    "    # Add P-S features\n",
    "    df[\"P_SUM\"] = df[payment_p].sum(axis=1) \n",
    "    df[\"S_SUM\"] = df[spend_p].sum(axis=1) \n",
    "    df[\"B_SUM\"] = df[balance_p].sum(axis=1)\n",
    "    df[\"P-S\"] = df.P_SUM - df.S_SUM       \n",
    "    df[\"P-B\"] = df.P_SUM - df.B_SUM\n",
    "    df=df.drop([\"S_SUM\",\"P_SUM\",\"B_SUM\"],axis=1)\n",
    "    print(f\"P-S feature added\")      \n",
    "\n",
    "\n",
    "    # Add Lag Columns \n",
    "    if CFG.ADD_LAG:\n",
    "      train_num_agg = df.groupby(\"customer_ID\")[num_cols].agg(['first', 'last']) #payment_p+balance_p+spend_p\n",
    "      train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "      train_num_agg.reset_index(inplace = True) \n",
    "      for col in train_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in train_num_agg:\n",
    "                    train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', 'first')]\n",
    "                    train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', 'first')]            \n",
    "      train_num_agg.drop([col for col in train_num_agg.columns if \"last\" in col],axis=1, inplace=True)\n",
    "      dgs.append(train_num_agg)\n",
    "      del train_num_agg\n",
    "      print(f\"Computing diff 1 features ,curr cols [{len(df.columns)}]\") \n",
    "      dff_cols =  payment_p+balance_p+spend_p+delq ## Replace with num_cols\n",
    "      \n",
    "\n",
    "      # add diff features\n",
    "      for pdf in CFG.ADD_DIFF:\n",
    "        train_diff = get_difference(df, dff_cols,period=pdf)\n",
    "        print(f\"Computing Diff {pdf} ,curr cols [{ train_diff.columns}]\") \n",
    "        dgs.append(train_diff)    \n",
    "        del train_diff; gc.collect()             \n",
    "    \n",
    "    # compute \"after pay\" features\n",
    "    for bcol in [f'B_{i}' for i in [11,14,17]]+['D_39','D_131']+[f'S_{i}' for i in [16,23]]:\n",
    "        for pcol in ['P_2','P_3']:\n",
    "            if bcol in df.columns:\n",
    "                df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
    "    # \n",
    "    df['S_2'] = pd.to_datetime(df['S_2'])\n",
    "    df['cid'], _ = df.customer_ID.factorize()    \n",
    "\n",
    "    # Add sundays count as a feature \n",
    "    s2_count = df[df.S_2.dt.dayofweek == 6].groupby(\"customer_ID\")['S_2'].agg(['count']) \n",
    "    s2_count.columns = ['S_2_Sun_Count']\n",
    "    s2_count.reset_index(inplace = True)     \n",
    "    dgs.append(s2_count)\n",
    "    print(f\"sundays count added and calculated [{len(s2_count.columns)}]\") \n",
    "\n",
    "    # Add week of the month correlation check standard deviation\n",
    "    df['S_2_wk'] =  df['S_2'].dt.week\n",
    "    s2_count = df.groupby(\"customer_ID\")['S_2_wk'].agg(['std'])  \n",
    "    s2_count.columns = ['S_2_wk_std']\n",
    "    s2_count.reset_index(inplace = True)     \n",
    "    dgs.append(s2_count)\n",
    "    df=df.drop([\"S_2_wk\"],axis=1 )\n",
    "    print(f\"sundays count added and calculated [{len(s2_count.columns)}]\")        \n",
    "    del s2_count; gc.collect()     \n",
    "\n",
    "\n",
    "    if CFG.ADD_CAT:  # generate summary staistics for categorical variables\n",
    "      train_cat_agg = df.groupby(\"customer_ID\")[cat_cols].agg(['count', 'nunique', 'std','first']) \n",
    "      train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "      train_cat_agg.reset_index(inplace = True)     \n",
    "      dgs.append(train_cat_agg)\n",
    "      del train_cat_agg; gc.collect() \n",
    "      train_cat_mean = df.groupby(\"customer_ID\")[cat_cols_avg].agg(['mean']) \n",
    "      train_cat_mean.columns = ['_'.join(x) for x in train_cat_mean.columns]\n",
    "      train_cat_mean.reset_index(inplace = True)    \n",
    "      print(f\"Added cat mean cols [{train_cat_mean.columns}]\")   \n",
    "      dgs.append(train_cat_mean)\n",
    "      del train_cat_mean; gc.collect() \n",
    "      print(f\"CAT features added {len(df.columns)}\") \n",
    "\n",
    "    # Add s2 count as a feature ( Number of spends)\n",
    "    s2_count = df.groupby(\"customer_ID\")['S_2'].agg(['count']) \n",
    "    s2_count.columns = ['S_2_Count']\n",
    "    s2_count.reset_index(inplace = True)    \n",
    "    df = df.merge(s2_count, on='customer_ID', how='inner')\n",
    "    print(f\"Stats added and calculated [{len(s2_count.columns)}]\")    \n",
    "    del s2_count; gc.collect() \n",
    "\n",
    "    # get intermediate client information\n",
    "    if CFG.ADD_MIDDLE:\n",
    "      df_middle = df[df.S_2_Count > 2].groupby(['customer_ID'])[balance_p+payment_p+delq+spend_p].apply(lambda x: x.iloc[(len(x)+1)//2])   \n",
    "      df_middle.columns = [x+'_mid' for x in df_middle.columns]  \n",
    "      dgs.append(df_middle) \n",
    "      print(f\"Mid Cols added [{len(df_middle.columns)}]\")    \n",
    "      del df_middle; gc.collect() \n",
    "      \n",
    "    # restore the original row order by sorting row_id\n",
    "    df = df.sort_values('row_id')\n",
    "    df = df.drop(['row_id'],axis=1)\n",
    "\n",
    "    return df, dgs\n",
    "\n",
    "\n",
    "def get_not_used():  \n",
    "  return ['row_id', 'customer_ID', 'target', 'cid', 'S_2','D_103','D_139']    \n",
    "\n",
    "def add_stats_step(df, cols):\n",
    "    n = 50\n",
    "    dgs = []\n",
    "    for i in range(0,len(cols),n):\n",
    "        s = i\n",
    "        e = min(s+n, len(cols))\n",
    "        dg = add_stats_one_shot(df, cols[s:e])\n",
    "        dgs.append(dg)\n",
    "    return dgs\n",
    "\n",
    "stats = ['mean', 'min', 'max','std']  # aggregation\n",
    "def add_stats_one_shot(df, cols):\n",
    "    \n",
    "    dg = df.groupby('customer_ID').agg({col:stats for col in cols})\n",
    "    out_cols = []\n",
    "    for col in cols:\n",
    "        out_cols.extend([f'{col}_{s}' for s in stats])\n",
    "    dg.columns = out_cols\n",
    "    dg = dg.reset_index()\n",
    "    return dg\n",
    "\n",
    "# Get the difference\n",
    "def get_difference(data, num_features,period=1): \n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in  data.groupby(['customer_ID']):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(period).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + f'_diff{period}' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "#   save data\n",
    "# ====================================================\n",
    "train = pd.read_parquet(f'{CFG.INPUT}/train.parquet') \n",
    "train = process_data(train) \n",
    "trainl = pd.read_csv(f'../input/amex-default-prediction/train_labels.csv')\n",
    "trainl.target = trainl.target.astype('int8')  \n",
    "train = train.merge(trainl, on='customer_ID', how='left')\n",
    "train.to_pickle(f\"train_fe_v1.pickle\")\n",
    "print(\"Saving train FE to file\") \n",
    "\n",
    "\n",
    "test = pd.read_parquet(f'{CFG.INPUT}/test.parquet') \n",
    "test = process_data(test) \n",
    "test.to_pickle(f\"test_fe_v1.pickle\")\n",
    "print(\"Saving test FE to file\")   \n",
    "print('FE finished')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d149ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
